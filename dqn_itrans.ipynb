{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--is_training IS_TRAINING]\n",
      "                             [--model_id MODEL_ID] [--model MODEL]\n",
      "                             [--data DATA] [--root_path ROOT_PATH]\n",
      "                             [--data_path DATA_PATH] [--features FEATURES]\n",
      "                             [--target TARGET] [--freq FREQ]\n",
      "                             [--checkpoints CHECKPOINTS] [--seq_len SEQ_LEN]\n",
      "                             [--label_len LABEL_LEN] [--pred_len PRED_LEN]\n",
      "                             [--enc_in ENC_IN] [--dec_in DEC_IN]\n",
      "                             [--c_out C_OUT] [--d_model D_MODEL]\n",
      "                             [--n_heads N_HEADS] [--e_layers E_LAYERS]\n",
      "                             [--d_layers D_LAYERS] [--d_ff D_FF]\n",
      "                             [--moving_avg MOVING_AVG] [--factor FACTOR]\n",
      "                             [--distil] [--dropout DROPOUT] [--embed EMBED]\n",
      "                             [--activation ACTIVATION] [--output_attention]\n",
      "                             [--do_predict] [--num_workers NUM_WORKERS]\n",
      "                             [--itr ITR] [--train_epochs TRAIN_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--patience PATIENCE]\n",
      "                             [--learning_rate LEARNING_RATE] [--des DES]\n",
      "                             [--loss LOSS] [--lradj LRADJ] [--use_amp]\n",
      "                             [--use_gpu USE_GPU] [--gpu GPU] [--use_multi_gpu]\n",
      "                             [--devices DEVICES] [--exp_name EXP_NAME]\n",
      "                             [--efficient_training EFFICIENT_TRAINING]\n",
      "                             [--channel_independence CHANNEL_INDEPENDENCE]\n",
      "                             [--inverse] [--class_strategy CLASS_STRATEGY]\n",
      "                             [--target_root_path TARGET_ROOT_PATH]\n",
      "                             [--target_data_path TARGET_DATA_PATH]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\czy\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-15232vQ6d419snPRw.json could match --features, --freq, --factor\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing DataLoaders for each model. These models include rule-based, vanilla DQN and encoder-decoder DQN.\n",
    "from DataLoader.DataLoader import YahooFinanceDataLoader\n",
    "from DataLoader.DataForPatternBasedAgent import DataForPatternBasedAgent\n",
    "from DataLoader.DataAutoPatternExtractionAgent import DataAutoPatternExtractionAgent\n",
    "from DataLoader.DataSequential import DataSequential \n",
    "\n",
    "from DeepRLAgent.MLPEncoder.Train import Train as SimpleMLP\n",
    "from DeepRLAgent.SimpleCNNEncoder.Train import Train as SimpleCNN\n",
    "from EncoderDecoderAgent.GRU.Train import Train as gru\n",
    "from EncoderDecoderAgent.CNN.Train import Train as cnn\n",
    "from EncoderDecoderAgent.CNN2D.Train import Train as cnn2d\n",
    "from EncoderDecoderAgent.CNNAttn.Train import Train as cnn_attn\n",
    "from EncoderDecoderAgent.CNN_GRU.Train import Train as cnn_gru\n",
    "\n",
    "\n",
    "# Imports for Deep RL Agent\n",
    "from DeepRLAgent.VanillaInput.Train import Train as DeepRL\n",
    "\n",
    "\n",
    "\n",
    "# Imports for RL Agent with n-step SARSA\n",
    "# from RLAgent.Train import Train as RLTrain\n",
    "\n",
    "# Imports for Rule-Based\n",
    "from PatternDetectionInCandleStick.LabelPatterns import label_candles\n",
    "from PatternDetectionInCandleStick.Evaluation import Evaluation\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from kaleido.scopes.plotly import PlotlyScope\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from EncoderDecoderAgent.iTransformer.Train import Train as iTransformer\n",
    "import argparse\n",
    "\n",
    "CURRENT_PATH = os.getcwd()\n",
    "BATCH_SIZE = 10\n",
    "GAMMA=0.7\n",
    "n_step = 10\n",
    "BATCH_SIZE = 10\n",
    "EPS = 0.1\n",
    "ReplayMemorySize = 20\n",
    "TARGET_UPDATE = 5\n",
    "# window_size = 20\n",
    "num_episodes = 3\n",
    "n_classes = 64\n",
    "hidden_size = 16\n",
    "initial_investment = 1000\n",
    "# Agent with Auto pattern extraction\n",
    "# State Mode\n",
    "state_mode = 1  # OHLC\n",
    "# state_mode = 2  # OHLC + trend\n",
    "# state_mode = 3  # OHLC + trend + %body + %upper-shadow + %lower-shadow\n",
    "window_size = 15\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "train_portfolios = {}\n",
    "test_portfolios = {}\n",
    "window_size_experiment = {}\n",
    "window_sizes = [3, 5, 8, 10, 12, 15, 20, 25, 30, 40, 50, 75]\n",
    "\n",
    "def add_train_portfo(model_name, portfo):\n",
    "    counter = 0\n",
    "    key = f'{model_name}'\n",
    "    while key in train_portfolios.keys():\n",
    "        counter += 1\n",
    "        key = f'{model_name}{counter}'\n",
    "        \n",
    "    train_portfolios[key] = portfo\n",
    "\n",
    "def add_test_portfo(model_name, portfo):\n",
    "    counter = 0\n",
    "    key = f'{model_name}'\n",
    "    while key in test_portfolios.keys():\n",
    "        counter += 1\n",
    "        key = f'{model_name}{counter}'\n",
    "    \n",
    "    test_portfolios[key] = portfo\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "# GOOGL\n",
    "\n",
    "DATASET_NAME = 'GOOGL'\n",
    "DATASET_FOLDER = 'GOOGL'\n",
    "FILE = 'GOOGL.csv'\n",
    "data_loader = YahooFinanceDataLoader(DATASET_FOLDER, '2018-01-01', load_from_file=True)\n",
    "transaction_cost = 0.0\n",
    "\n",
    "\n",
    "dataTrain_autoPatternExtractionAgent = DataAutoPatternExtractionAgent(data_loader.data_train, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTest_autoPatternExtractionAgent = DataAutoPatternExtractionAgent(data_loader.data_test, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTrain_patternBased = DataForPatternBasedAgent(data_loader.data_train, data_loader.patterns, 'action_deepRL', device, GAMMA, n_step, BATCH_SIZE, transaction_cost)\n",
    "dataTest_patternBased = DataForPatternBasedAgent(data_loader.data_test, data_loader.patterns, 'action_deepRL', device, GAMMA, n_step, BATCH_SIZE, transaction_cost)\n",
    "\n",
    "dataTrain_sequential = DataSequential(data_loader.data_train,\n",
    "                           'action_encoder_decoder', device, GAMMA,\n",
    "                           n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTest_sequential = DataSequential(data_loader.data_test,\n",
    "                          'action_encoder_decoder', device, GAMMA,\n",
    "                          n_step, BATCH_SIZE, window_size, transaction_cost)  \n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "parser = argparse.ArgumentParser(description='iTransformer')\n",
    "# # basic config\n",
    "    # parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "    # parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "    # parser.add_argument('--model', type=str, required=True, default='iTransformer',\n",
    "    #                     help='model name, options: [iTransformer, iInformer, iReformer, iFlowformer, iFlashformer]')\n",
    "parser.add_argument('--is_training', type=int,  default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str,  default='iTransformer',\n",
    "                    help='model name, options: [iTransformer, iInformer, iReformer, iFlowformer, iFlashformer]')\n",
    "\n",
    "# data loader\n",
    "# parser.add_argument('--data', type=str, required=True, default='custom', help='dataset type')\n",
    "parser.add_argument('--data', type=str, default='custom', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/electricity/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='electricity.csv', help='data csv file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=15, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length') # no longer needed in inverted Transformers\n",
    "parser.add_argument('--pred_len', type=int, default=1, help='prediction sequence length')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size') # applicable on arbitrary number of variates in inverted Transformers\n",
    "parser.add_argument('--d_model', type=int, default=16, help='dimension of model') # token维度\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=1, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# iTransformer\n",
    "parser.add_argument('--exp_name', type=str, required=False, default='None',\n",
    "                    help='experiemnt name, options:[partial_train, zero_shot]')\n",
    "parser.add_argument('--efficient_training', type=bool, default=False, help='whether to use efficient_training (exp_name should be partial train)')\n",
    "parser.add_argument('--channel_independence', type=bool, default=False, help='whether to use channel_independence mechanism')\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "parser.add_argument('--class_strategy', type=str, default='projection', help='projection/average/cls_token')\n",
    "parser.add_argument('--target_root_path', type=str, default='./data/electricity/', help='root path of the data file')\n",
    "parser.add_argument('--target_data_path', type=str, default='electricity.csv', help='data file')\n",
    "\n",
    "args = parser.parse_args()\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "itrans = iTransformer(data_loader, dataTrain_sequential, dataTest_sequential, DATASET_NAME, transaction_cost,\n",
    "                    config=args,\n",
    "                    hidden_size=hidden_size,\n",
    "                    BATCH_SIZE=BATCH_SIZE, GAMMA=GAMMA,\n",
    "                    ReplayMemorySize=ReplayMemorySize,\n",
    "                    TARGET_UPDATE=TARGET_UPDATE,\n",
    "                    n_step=n_step,\n",
    "                    window_size=window_size)\n",
    "# ↓原来的\n",
    "# simpleMLP = SimpleMLP(data_loader, dataTrain_patternBased, dataTest_patternBased, DATASET_NAME, \n",
    "#                     state_mode, window_size, transaction_cost, n_classes, BATCH_SIZE=BATCH_SIZE, GAMMA=GAMMA, EPS=EPS,\n",
    "#                     ReplayMemorySize=ReplayMemorySize, TARGET_UPDATE=TARGET_UPDATE, n_actions=n_actions, n_step=n_step)\n",
    "\n",
    "itrans.train(num_episodes)\n",
    "file_name = None\n",
    "\n",
    "file_name = 'GOOGL; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl'\n",
    "# file_name = 'AAPL; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl'\n",
    "# file_name = 'KSS; DATA_KIND(AutoPatternExtraction); BEGIN_DATE(None); END_DATE(None); SPLIT_POINT(2018-01-01); MLP; TC(0.0); StateMode(1); WindowSize(3); BATCH_SIZE10; GAMMA0.7; EPS0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10(1).pkl'\n",
    "# file_name = 'BTC-USD; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl'\n",
    "# file_name = 'GE; DATA_KIND(AutoPatternExtraction); BEGIN_DATE(None); END_DATE(None); SPLIT_POINT(2015-01-01); MLP; TC(0); StateMode(1); WindowSize(20); BATCH_SIZE10; GAMMA0.7; EPS0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10.pkl'\n",
    "# file_name = 'HSI; DATA_KIND(AutoPatternExtraction); BEGIN_DATE(None); END_DATE(None); SPLIT_POINT(2015-01-01); MLP; TC(0); StateMode(1); WindowSize(20); BATCH_SIZE10; GAMMA0.7; EPS0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10.pkl'\n",
    "# file_name = 'AAL; DATA_KIND(AutoPatternExtraction); BEGIN_DATE(None); END_DATE(None); SPLIT_POINT(2018-01-01); MLP; TC(0); StateMode(1); WindowSize(15); BATCH_SIZE10; GAMMA0.7; EPS0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10.pkl'\n",
    "\n",
    "ev_iTransformer_train = itrans.test(\n",
    "                                  initial_investment=initial_investment, test_type='train')\n",
    "# ↓原来的\n",
    "# ev_simpleMLP = simpleMLP.test(file_name=file_name, action_name=dataTrain_patternBased.action_name,\n",
    "#                                   initial_investment=initial_investment, test_type='train')\n",
    "simpleMLP_portfolio_train = ev_iTransformer_train.get_daily_portfolio_value()\n",
    "\n",
    "ev_iTransformer_test = itrans.test(\n",
    "                                  initial_investment=initial_investment, test_type='test')\n",
    "simpleMLP_portfolio_test = ev_iTransformer_test.get_daily_portfolio_value()\n",
    "\n",
    "model_kind = 'iTransformer'\n",
    "# model_kind = 'MLP'\n",
    "\n",
    "add_train_portfo(model_kind, simpleMLP_portfolio_train)\n",
    "add_test_portfo(model_kind, simpleMLP_portfolio_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
